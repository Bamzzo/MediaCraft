# app/rag.py
import os
import time
import random
import requests
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

knowledge_progress = {}

RERANK_MODEL = "BAAI/bge-reranker-v2-m3"
RERANK_URL = "https://api.siliconflow.cn/v1/rerank"


def get_embeddings():
    """åˆå§‹åŒ– Embedding æ¨¡å‹ï¼Œä½¿ç”¨ç¡…åŸºæµåŠ¨ BAAI/bge-m3"""
    return OpenAIEmbeddings(
        model="BAAI/bge-m3",
        api_key=os.getenv("SILICONFLOW_API_KEY"),
        base_url="https://api.siliconflow.cn/v1",
        chunk_size=50,
    )


def get_vector_store():
    """åˆå§‹åŒ–å¹¶è·å–æœ¬åœ° ChromaDB å‘é‡åº“å®ä¾‹"""
    persist_directory = os.path.join(os.path.dirname(os.path.dirname(__file__)), "chroma_db")
    return Chroma(
        collection_name="bytecreator_knowledge",
        embedding_function=get_embeddings(),
        persist_directory=persist_directory,
    )


def add_to_knowledge_base(text: str, source: str = "manual_input"):
    """å¸¦è‡ªåŠ¨é‡è¯•æœºåˆ¶çš„å…¥åº“å¼•æ“"""
    print(f"ğŸ“š æ­£åœ¨å¤„ç†å¹¶åˆ‡åˆ†æ–‡æœ¬ï¼Œæ¥æº: {source}...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""],
    )
    valid_chunks = [chunk for chunk in text_splitter.split_text(text) if chunk.strip()]
    documents = [Document(page_content=chunk, metadata={"source": source}) for chunk in valid_chunks]

    vector_store = get_vector_store()
    batch_size = 50
    total_docs = len(documents)
    knowledge_progress[source] = {"current": 0, "total": total_docs, "status": "processing"}
    print(f"ğŸ“¦ å…±è®¡ {total_docs} ä¸ªæœ‰æ•ˆçŸ¥è¯†å—ï¼Œå¼€å§‹åˆ†æ‰¹å®‰å…¨å…¥åº“...")

    for i in range(0, total_docs, batch_size):
        batch = documents[i : i + batch_size]
        max_retries = 5
        for attempt in range(max_retries):
            try:
                vector_store.add_documents(batch)
                knowledge_progress[source]["current"] = min(i + batch_size, total_docs)
                print(f"âœ… å…¥åº“è¿›åº¦: {min(i + batch_size, total_docs)} / {total_docs}")
                time.sleep(0.5)
                break
            except Exception as e:
                if "429" in str(e) and attempt < max_retries - 1:
                    wait_time = (2**attempt) + random.random()
                    print(f"âš ï¸ è§¦å‘é™æµï¼Œç­‰å¾… {wait_time:.1f}s åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ æ‰¹é‡å…¥åº“å¤±è´¥: {e}")
                    raise

    knowledge_progress[source]["status"] = "completed"
    print(f"âœ… æˆåŠŸå°† {total_docs} ä¸ªæ–‡æœ¬å—å­˜å…¥ ChromaDB å‘é‡åº“ï¼(ä¿å­˜åœ¨ chroma_db ç›®å½•)")
    return total_docs


def _rerank_documents(query: str, docs: list[Document], top_k: int) -> list[Document]:
    """è°ƒç”¨ç¡…åŸºæµåŠ¨ BAAI/bge-reranker-v2-m3 å¯¹å€™é€‰æ–‡æ¡£æ‰“åˆ†å¹¶è¿”å› top_k æ¡"""
    api_key = os.getenv("SILICONFLOW_API_KEY")
    if not api_key:
        return docs[:top_k]
    documents_text = [d.page_content for d in docs]
    payload = {
        "model": RERANK_MODEL,
        "query": query,
        "documents": documents_text,
        "top_n": top_k,
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    try:
        resp = requests.post(RERANK_URL, json=payload, headers=headers, timeout=15)
        if resp.status_code != 200:
            return docs[:top_k]
        data = resp.json()
        results = data.get("results") or []
        if not results:
            return docs[:top_k]
        # results æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½ï¼Œæ¯é¡¹å« indexï¼ˆåœ¨ docs ä¸­çš„ä¸‹æ ‡ï¼‰
        indices = [r["index"] for r in results[:top_k] if 0 <= r["index"] < len(docs)]
        return [docs[i] for i in indices]
    except Exception as e:
        print(f"âš ï¸ Rerank API è°ƒç”¨å¤±è´¥ï¼Œå›é€€ä¸ºå‘é‡å‰ k æ¡: {e}")
        return docs[:top_k]


def query_knowledge_base(query: str, k: int = 3) -> str:
    """æµ·é€‰ + ç²¾é€‰ (Rerank) æ£€ç´¢æ¶æ„ï¼Œé‡æ’åºå¤±è´¥æ—¶å›é€€ä¸ºå‰ k æ¡"""
    try:
        vector_store = get_vector_store()
        initial_results = vector_store.similarity_search(query, k=10)
        if not initial_results:
            print("âš ï¸ çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°é«˜åº¦ç›¸å…³çš„ç‰‡æ®µã€‚")
            return ""

        print(f"ğŸ” [Rerank] æ­£åœ¨å¯¹ {len(initial_results)} æ¡å€™é€‰çŸ¥è¯†è¿›è¡Œç²¾é€‰...")
        try:
            final_docs = _rerank_documents(query, initial_results, k)
        except Exception as rerank_err:
            print(f"âš ï¸ é‡æ’åºå¤±è´¥ï¼Œå›é€€ä¸ºåŸå§‹å‰ {k} æ¡: {rerank_err}")
            final_docs = initial_results[:k]

        context_pieces = [
            f"ã€æ¥æº: {d.metadata.get('source', 'æœªçŸ¥')}ã€‘\n{d.page_content}" for d in final_docs
        ]
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_docs)} ä¸ªç›¸å…³ç‰‡æ®µã€‚")
        return "\n\n---\n\n".join(context_pieces)

    except Exception as e:
        print(f"âŒ æ£€ç´¢å¼‚å¸¸: {e}")
        return ""
