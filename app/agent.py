# app/agent.py
import os
import operator
from typing import Annotated, Sequence, TypedDict

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from app.tools import tools


# --- ğŸ­ Model Factory (æ ¸å¿ƒå·¥å‚) ---
def get_llm(model_label: str):
    """æ ¹æ®å‰ç«¯ä¼ æ¥çš„æ ‡ç­¾ï¼Œè¿”å›å¯¹åº”çš„ LLM å®ä¾‹"""

    print(f"ğŸ­ åˆå§‹åŒ–æ¨¡å‹: {model_label}")

    # 1. å®˜æ–¹åŸç”Ÿ DeepSeek (ç›´è¿é…ç½®)
    if "DeepSeek" in model_label:
        return ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com",
            temperature=0.7,
            streaming=True,
        )

    # 2. Llama (NVIDIA å®˜æ–¹ NIM ç›´è¿ï¼Œæ»¡è¡€ 70B)
    elif "Llama" in model_label:
        return ChatOpenAI(
            model="meta/llama-3.1-70b-instruct",
            api_key=os.getenv("NVIDIA_API_KEY"),
            base_url="https://integrate.api.nvidia.com/v1",
            temperature=0.6,
            streaming=True,
        )

    # 2. Volcengine (å­—èŠ‚è·³åŠ¨ - è±†åŒ…)
    elif "Doubao" in model_label:
        endpoint_id = os.getenv("DOUBAO_LLM_ENDPOINT")
        if not endpoint_id:
            print("âš ï¸ è­¦å‘Š: æœªé…ç½® DOUBAO_LLM_ENDPOINTï¼Œå›é€€åˆ° DeepSeek")
            return get_llm("DeepSeek")

        return ChatOpenAI(
            model=endpoint_id,
            api_key=os.getenv("VOLC_API_KEY"),
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            temperature=0.7,
            streaming=True,
        )

    # 3. ZhipuAI (GLM-4-Plus)
    elif "GLM" in model_label:
        return ChatOpenAI(
            model="glm-4-plus",
            api_key=os.getenv("ZHIPU_API_KEY"),
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            temperature=0.7,
            streaming=True,
        )

    # --- ğŸ‘ï¸ ç¥ä¹‹çœ¼ï¼šè§†è§‰è§£æå¤§æ¨¡å‹ ---
    elif "Qwen2-VL" in model_label:
        return ChatOpenAI(
            model="Qwen/Qwen2-VL-72B-Instruct",
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            base_url="https://api.siliconflow.cn/v1",
            temperature=0.7,
            streaming=True,
        )

    elif "GLM-4V" in model_label:
        return ChatOpenAI(
            model="glm-4v-plus",
            api_key=os.getenv("ZHIPU_API_KEY"),
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            temperature=0.7,
            streaming=True,
        )

    elif "Qwen" in model_label:
        return ChatOpenAI(
            model="Qwen/Qwen2.5-72B-Instruct",
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            base_url="https://api.siliconflow.cn/v1",
            temperature=0.7,
            streaming=True,
        )

    # é»˜è®¤å…œåº•
    print(f"âš ï¸ æœªçŸ¥æ¨¡å‹æ ‡ç­¾ [{model_label}]ï¼Œé™çº§ä½¿ç”¨ DeepSeek-V3")
    return get_llm("DeepSeek")


# --- State å®šä¹‰ ---
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]


# --- Nodes (èŠ‚ç‚¹é€»è¾‘) ---
def call_model(state: AgentState, config: RunnableConfig):
    messages = state["messages"]

    configurable = config.get("configurable", {})
    selected_chat_model = configurable.get("selected_chat_model", "DeepSeek")
    system_prompt_text = configurable.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚")

    # å¤§è„‘æ°¸è¿œæ˜¯æ–‡æœ¬æ¨¡å‹ï¼Œå›¾ç‰‡é€šè¿‡ analyze_uploaded_image å·¥å…·æŸ¥çœ‹
    identity_prompt = f"\n\n[ç³»ç»ŸæŒ‡ä»¤ï¼šä½ æ˜¯åŸºäº {selected_chat_model} é©±åŠ¨çš„æ ¸å¿ƒå¤§è„‘ã€‚å¦‚æœç”¨æˆ·é™„å¸¦äº†å›¾ç‰‡æˆ–è§†é¢‘ï¼Œä½ å¿…é¡»åˆ†åˆ«è°ƒç”¨ analyze_uploaded_image æˆ– analyze_uploaded_video å·¥å…·æ¥è¿›è¡Œè§†è§‰æ„ŸçŸ¥ã€‚]"
    sys_msg = SystemMessage(content=system_prompt_text + identity_prompt)

    prompt_messages = [sys_msg] + messages
    llm = get_llm(selected_chat_model)
    llm_with_tools = llm.bind_tools(tools)
    response = llm_with_tools.invoke(prompt_messages)

    return {"messages": [response]}


def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    if getattr(last_message, "tool_calls", None):
        return "tools"
    return END


# --- Graph æ„å»º ---
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("tools", ToolNode(tools))

workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {"tools": "tools", END: END},
)
workflow.add_edge("tools", "agent")

# ğŸ§  æ¤å…¥æµ·é©¬ä½“ (MemorySaver)
memory = MemorySaver()
app_graph = workflow.compile(checkpointer=memory)
